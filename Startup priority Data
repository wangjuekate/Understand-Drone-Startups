import html2text
import selenium
import time
import requests
import urllib2
import time
import json
import urllib2
import requests
import csv
import bs4
import smtplib
import pprint
import xlrd
import csv
import string
from googlesearch import search
from xlrd import open_workbook
from selenium import webdriver
import string
import re
# In Python

# write out the information
csvfile = open('C:/Users/wang547/addinformation.csv', 'wb')
fieldnames = ['DocID', 'firmname', 'indexpage']
writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
writer.writeheader()

profile = webdriver.FirefoxProfile()
profile.set_preference("browser.download.folderList", 2)
# profile.set_preference("browser.download.manager.showWhenStarting", False)
profile.set_preference("browser.helperApps.alwaysAsk.force", False)
browser = webdriver.Firefox(profile)
# Read FileID
book = open_workbook('C:\Users\wang547\Documents\DocID.xlsx')
for i in range(9888, 10000):
    item =[]
    print i
    sheet1 = book.sheet_by_index(0)
    address = ''.join(str(sheet1.cell(i, 0).value))
    website = 'https://www.regulations.gov' + '/document?D=FAA-' + address + '-0001'
    browser.get(website)
    time.sleep(5)

# Extract Name from the website
    firmname = browser.find_element_by_xpath('//h1[@class="GIY1LSJBID"]').text.encode('utf-8')
# Extract Name from the website
    k = firmname.split('-')[0]
    k = k.translate(None,string.punctuation)
    k= k.replace("'","")
    k = k.replace("-", "")
    checkname = k.split(' ')[0].lower()
    print checkname


# use Google API to get the index page of the startup

    for firmurl in search(k, tld='com.pk', lang='eng', stop=1, num=1):
        firmurl= firmurl
        firmurl = firmurl.split('/')[2]
        print firmurl

# check whether the website is valid
    if checkname not in firmurl:
        firmurl = ""
    else:
        worktime = '201603'
        url = 'https://web.archive.org/web/' + worktime + '/https://' + firmurl
        try:
            page = urllib2.urlopen(url)
            try:
                f = page.read().decode('utf-8')
                html = f
                h = html2text.HTML2Text()
                h.ignore_links = True
                text = h.handle(html).encode('utf-8')
            except UnicodeDecodeError:
                text =""
        except urllib2.HTTPError:
            text =''
    # write out to files, name as FileID with time
        file = 'C:/Users/wang547/' + address +'-'+ worktime + '.txt'

        out = open(file, 'w+')
        out.write(text)

        for addfirmurl in search(k+' wikipedia', tld='com.pk', lang='eng', stop=1, num=1):
            addfirmurl = addfirmurl

        # check whether the website is valid
        if (checkname in addfirmurl.lower()) & ('wikipedia' in addfirmurl.lower()):
            page = urllib2.urlopen(addfirmurl)
            try:
                f = page.read().decode('utf-8')
                html = f
                h = html2text.HTML2Text()
                h.ignore_links = True
                text = h.handle(html).encode('utf-8')
            except UnicodeDecodeError:
                text = ""


                # write out to files, name as FileID with time
            file = 'C:/Users/wang547/' + address +'-'+ worktime + 'wiki.txt'
            print file
            out = open(file, 'w+')
            out.write(text)
    item.append(address)
    item.append(k)
    item.append(firmurl)
    print item
    f = open('C:/Users/wang547/applicationinfor.txt','a')
    json.dump(item,f)


#additional files to check
1300
1316
2128
2966
5138
5475
5544
5551
5661
5800
6620
7142
7507
7622
7752
8593

# R code to clean
data<-readLines("C:\\Users\\wang547\\applicationinfor.txt")
data1<-unlist(strsplit(data,"]"))
cleanup<-function(input){
results<-unlist(strsplit(input,","))
results<-gsub("[[:punct:]]"," ",results)
ID<-unlist(strsplit(results[1]," "))
ID<-ID[which(ID!="")]
ID<-paste(ID,collapse="-")
Name<-results[2]
website<-results[3]
output<-data.frame(ID)
output<-cbind(output,Name,website)
return(output)
}
library(plyr)
results<-lapply(data1,cleanup)
library(data.table)
results<-rbindlist(results)
write.table(final,file="firmwebsite.csv",sep=",",row.names=FALSE)

#read file names 

wikifiles<-list.files(path = "C:\\Users\\wang547\\", pattern = "wiki.txt")
wikifiles<-gsub("-201603wiki.txt","",wikifiles)
results$Public<-0
results$ID<-as.character(as.matrix(results$ID))
results<-data.frame(results)
results[which(results$ID%in%wikifiles),]$Public<-1

#file names
file<-list.files(path = "C:\\Users\\wang547\\", pattern = "-201603.txt")
clean<-function(input){
words<-readLines(paste("C:\\Users\\wang547\\",input[1],sep=""))
ID<-gsub("-201603.txt","",input[1])
time<-words[15]
words<-tolower(words)
construction<-length(grep("construction",words))
oilgass<-length(grep("oil",words))
argriculture<-length(grep("agriculture",words))
output<-data.frame(ID)
output<-cbind(output,time,construction,oilgass,argriculture)
return(output)
}
library(plyr)
timeindustry<-lapply(file,clean)
timeindustry<-rbindlist(timeindustry)
timeindustry$ID<-as.character(as.matrix(timeindustry$ID))
timeindustry<-data.frame(timeindustry)
final<-merge(results,timeindustry,by.x="ID",by.y="ID",all.x=TRUE)
