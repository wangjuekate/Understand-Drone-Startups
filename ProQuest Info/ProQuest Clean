
#there are four steps:
#1.Python code the links 2. R clean the links 3.Python request the content links 4. R clean up the content 5. combine pieces together
#saved data FinalQuest.RData

#Python links
from selenium import webdriver
from selenium.common.exceptions import TimeoutException
from selenium.common.exceptions import NoSuchElementException
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
import time
from bs4 import BeautifulSoup
import requests
import json
profile = webdriver.FirefoxProfile()
profile.set_preference("browser.download.folderList", 2)
profile.set_preference("browser.download.dir", "C:\Users\wang547\Downloads\Containdownload")
# profile.set_preference("browser.download.manager.showWhenStarting", False)
profile.set_preference("browser.helperApps.alwaysAsk.force", False)



browser = webdriver.Firefox(profile)

browser.get('https://search-proquest-com.libproxy2.usc.edu/?accountid=14749')
timeout = 5
try:

    search =browser.find_element_by_xpath('//*[@id="searchTerm"]')
    search.send_keys('FT(drone OR (Unmanned /N0 Aerial /N0 Vehicle) OR (Unmanned /N0 Air /N0 Vehicle) OR (Unmanned /N0 Aircraft /N0 Vehicle) OR (Unmanned /N0 Aerospace /N0 Vehicle) OR (Uninhabited /N0 Aircraft /N0 Vehicle)) AND PD(>2015-09-01 AND <2016-01-01)' )
    htmlElem = browser.find_element_by_xpath('//*[@id="expandedSearch"]')
    htmlElem.send_keys(Keys.ENTER)
    time.sleep(120)
except TimeoutException:
    print("Timed out waiting for page to load")

for i in range(1000, 5000,1):
    print i
    soup = BeautifulSoup(browser.page_source)
    sources = soup.find_all('a', {'id': 'citationDocTitleLink'})
    for source in sources:
        item = []
        item.append(source['href'])
        f = open('C:/Users/wang547/Downloads/index3.txt', 'a')
        json.dump(item, f)
    next = browser.find_element_by_xpath('//*[@id="mainContentRight"]/nav/ul/li[13]/a')
    next.click()
    time.sleep(15)



#R clean links

data<-readLines("C:\\Users\\wang547\\Downloads\\index15.txt")
output<-unlist(strsplit(data,"]"))
cleanid<-function(input){
add<-unlist(strsplit(input,"[[:punct:]]"))
id<-add[10]
class<-add[11]
account<-add[14]
output<-data.frame(id)
output<-cbind(output,class,account)
return(output)
}
library(plyr)
results<- lapply(output,cleanid)
results<-do.call(rbind,results)
write.table(results,file="index15.csv",row.names=FALSE,sep=",")

#Python content
from selenium import webdriver
from selenium.common.exceptions import TimeoutException
from selenium.common.exceptions import NoSuchElementException
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
import time
from bs4 import BeautifulSoup
import requests
import json
import csv
from xlrd import open_workbook

profile = webdriver.FirefoxProfile()
profile.set_preference("browser.download.folderList", 2)
profile.set_preference("browser.download.dir", "C:\Users\wang547\Downloads\Containdownload")
# profile.set_preference("browser.download.manager.showWhenStarting", False)
profile.set_preference("browser.helperApps.alwaysAsk.force", False)

book = open_workbook('C:/Users/wang547/Documents/allindex.xlsx')

for i in range(13269,300000):
    print i
    id = int(book.sheet_by_index(0).cell(i, 0).value)
    print id
    address = book.sheet_by_index(0).cell(i, 1).value
    account = book.sheet_by_index(0).cell(i, 2).value
    url = 'https://search.proquest.com/docview/' + str(id) + '/' + 'abstract/' + str(address) + '/1?accountid=14749'
    print url
    soup = BeautifulSoup(requests.get(url).text)
    item = ["NA"]*6
    itemfile = soup.find_all('div', {'class': 'display_record_indexing_row'})
    itempt = []

    for itemf in itemfile:
        ifind = itemf.find('div', {'class': 'display_record_indexing_fieldname'})
        name = ifind.text.encode('utf-8')
        if name == "Subject ":
            subject = itemf.find('div', {'class': 'display_record_indexing_data'})
            subject = subject.text.encode('utf-8')
            item[0]=subject
        else:
            if name == "Publication date ":
                subject = itemf.find('div', {'class': 'display_record_indexing_data'})
                subject = subject.text.encode('utf-8')
                item[1] =subject
            else:

                if name == "Place of publication ":
                    subject = itemf.find('div', {'class': 'display_record_indexing_data'})
                    subject = subject.text.encode('utf-8')
                    item[2] =subject
                else:

                    if name == "ProQuest document ID ":
                        subject = itemf.find('div', {'class': 'display_record_indexing_data'})
                        subject = subject.text.encode('utf-8')
                        item[3] =subject
                    else:
                        if name == "Source type ":
                            subject = itemf.find('div', {'class': 'display_record_indexing_data'})
                            subject = subject.text.encode('utf-8')
                            item[4] =subject
    url2 = 'https://search.proquest.com/docview/' + str(id) + '/' + 'fulltext/' + str(address) + '/1?accountid=14749'
    soup = BeautifulSoup(requests.get(url2).text)
    text = soup.find('div', {'id': 'readableContent'})
    if text is not None:
        text = text.find_all('p')
        txt = ""
        for i in text:
            txt1 = i.text.encode('utf-8')
            if txt1 is not None:
                txt = txt + txt1
        item[5] =txt
    f = open('C:/Users/wang547/Downloads/allindex.txt', 'a')
    json.dump(item, f)



#R clean content
text<-readLines("C:\\Users\\wang547\\Downloads\\index4\\index4content.txt")
output<-unlist(strsplit(text,'"]'))

cleanid<-function(input){
add<-unlist(strsplit(as.character(input),"\""))
add<-add[which(add!="")]
add<-gsub("[[:punct:]]"," ",add)
add<-add[which(add!=" ")]
add<-add[which(add!="  ")]
subject<-add[1]
time<-add[2]
location<-add[3]
id<-add[4]
source<-add[5]
txt<-paste(add[6:length(add)],collapse=" ")
output<-data.frame(subject)
output<-cbind(output,time,location,id,source,txt)
return(output)
}
library(plyr)
results<- lapply(output,cleanid)
library(data.table)
results<-rbindlist(results)
index3<-results
save(index3,file="index4.RData")
