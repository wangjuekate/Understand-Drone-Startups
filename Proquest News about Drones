from selenium import webdriver
from selenium.common.exceptions import TimeoutException
from selenium.common.exceptions import NoSuchElementException
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
import time
from bs4 import BeautifulSoup
import requests
import json
profile = webdriver.FirefoxProfile()
profile.set_preference("browser.download.folderList", 2)
profile.set_preference("browser.download.dir", "C:\Users\wang547\Downloads\Containdownload")
# profile.set_preference("browser.download.manager.showWhenStarting", False)
profile.set_preference("browser.helperApps.alwaysAsk.force", False)

book=open_workbook('C:\Users\wang547\Documents\index1.csv')

for i in range(0,1000):
    print i
    id = book.sheet_by_index(0).cell(i,0).value
    address =book.sheet_by_index(0).cell(i,1).value
    account = book.sheet_by_index(0).cell(i,2).value
    url = 'https://search.proquest.com/docview/'+id+'/'+address+'/'+'/1?accountid=14749'
    soup=BeautifulSoup(browser.page_source)
    sources = soup.find_all('div',{'class':'item clearfix'})
    for source in sources:
        item = []
        herf1 =source.find('a', {'id': 'addFlashPageParameterformat_citation'})
        if herf1 is None:
            herf1 = source.find('a', {'id': 'addFlashPageParameterformat_abstract'})
            if herf1 is not None:
                address = herf1['href']
                print address
                item.append(''.join(address))
                soup = BeautifulSoup(requests.get(address).text)
                itemfile = soup.find_all('div', {'class': 'display_record_indexing_fieldname'})
                itempt = []
                for itemf in itemfile:
                    name = itemf.text.encode('utf-8')
                    if name is not None:
                        itempt.append(''.join(name))
                    else:
                        itempt.append(' ')
                itemfile = soup.find_all('div', {'class': 'display_record_indexing_data'})
                itempt2 = []
                for itemf in itemfile:
                    name = itemf.text.encode('utf-8')
                    if name is not None:
                        itempt2.append(''.join(name))
                    else:
                        itempt2.append(' ')
        #select useful information
                try:
                    idcheck = itempt.index("Subject ")
                    subject= itempt2[idcheck]
                except ValueError:
                    subject =""
                item.append(subject)
                try:
                    idcheck = itempt.index("Publication date ")
                    subject = itempt2[idcheck]
                except ValueError:
                    subject = ""
                item.append(subject)
                try:
                    idcheck = itempt.index("Place of publication ")
                    subject = itempt2[idcheck]
                except ValueError:
                    subject = ""
                item.append(subject)
                try:
                    idcheck = itempt.index("ProQuest document ID ")
                    subject = itempt2[idcheck]
                except ValueError:
                    subject = ""
                item.append(subject)
                try:
                    idcheck = itempt.index("Source type ")
                    subject = itempt2[idcheck]
                except ValueError:
                    subject = ""
                item.append(subject)
        herf2 = source.find('a', {'id': 'addFlashPageParameterformat_fulltext'})
        if herf2 is not None:
            address = herf2['href']
            soup = BeautifulSoup(requests.get(address).text)
            text = soup.find('div', {'id': 'readableContent'})
            text= text.find_all('p')
            txt=""
            for i in text:
                txt = i.text.encode('utf-8')
                if txt is not None:
                    txt = txt.join(txt)
        else:
            txt =""
        item.append(txt)
        f = open('C:/Users/wang547/Downloads/test.txt', 'a')
        json.dump(item, f)
    time.sleep(10)
    next = browser.find_element_by_xpath('//*[@id="mainContentRight"]/nav/ul/li[13]/a')
    next.click()
    time.sleep(5)

#R code clearning
data<-readLines("C:\\Users\\wang547\\Downloads\\index1.txt")
output<-unlist(strsplit(data,"]"))
cleanid<-function(input){
add<-unlist(strsplit(input,"[[:punct:]]"))
id<-add[10]
class<-add[11]
account<-add[14]
output<-data.frame(id)
output<-cbind(output,class,account)
return(output)
}
library(plyr)
results<- lapply(output,cleanid)
results<-do.call(rbind,results)
write.table(results,file="index1.csv",row.names=FALSE,sep=",")

#Python code get index
