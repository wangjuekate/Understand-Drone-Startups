from selenium import webdriver
from selenium.common.exceptions import TimeoutException
from selenium.common.exceptions import NoSuchElementException
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
import time
from bs4 import BeautifulSoup
import requests
import json
profile = webdriver.FirefoxProfile()
profile.set_preference("browser.download.folderList", 2)
profile.set_preference("browser.download.dir", "C:\Users\wang547\Downloads\Containdownload")
# profile.set_preference("browser.download.manager.showWhenStarting", False)
profile.set_preference("browser.helperApps.alwaysAsk.force", False)



browser = webdriver.Firefox(profile)

browser.get('https://search-proquest-com.libproxy2.usc.edu/?accountid=14749')
timeout = 5


try:

    search =browser.find_element_by_xpath('//*[@id="searchTerm"]')
    search.send_keys('FT(drone OR (Unmanned /N0 Aerial /N0 Vehicle) OR (Unmanned /N0 Air /N0 Vehicle) OR (Unmanned /N0 Aircraft /N0 Vehicle) OR (Unmanned /N0 Aerospace /N0 Vehicle) OR (Uninhabited /N0 Aircraft /N0 Vehicle)) AND PD(>2012-01-01 AND <2017-01-01)' )
    htmlElem = browser.find_element_by_xpath('//*[@id="expandedSearch"]')
    htmlElem.send_keys(Keys.ENTER)
    time.sleep(100)
except TimeoutException:
    print("Timed out waiting for page to load")

for i in range(0,1000):
    print i
    soup=BeautifulSoup(browser.page_source)
    sources = soup.find_all('div',{'class':'item clearfix'})
    for source in sources:
        item = []
        herf1 =source.find('a', {'id': 'addFlashPageParameterformat_citation'})
        if herf1 is None:
            herf1 = source.find('a', {'id': 'addFlashPageParameterformat_abstract'})
            if herf1 is not None:
                address = herf1['href']
                print address
                item.append(''.join(address))
                soup = BeautifulSoup(requests.get(address).text)
                itemfile = soup.find_all('div', {'class': 'display_record_indexing_fieldname'})
                itempt = []
                for itemf in itemfile:
                    name = itemf.text.encode('utf-8')
                    if name is not None:
                        itempt.append(''.join(name))
                    else:
                        itempt.append(' ')
                itemfile = soup.find_all('div', {'class': 'display_record_indexing_data'})
                itempt2 = []
                for itemf in itemfile:
                    name = itemf.text.encode('utf-8')
                    if name is not None:
                        itempt2.append(''.join(name))
                    else:
                        itempt2.append(' ')
        #select useful information
                try:
                    idcheck = itempt.index("Subject ")
                    subject= itempt2[idcheck]
                except ValueError:
                    subject =""
                item.append(subject)
                try:
                    idcheck = itempt.index("Publication date ")
                    subject = itempt2[idcheck]
                except ValueError:
                    subject = ""
                item.append(subject)
                try:
                    idcheck = itempt.index("Place of publication ")
                    subject = itempt2[idcheck]
                except ValueError:
                    subject = ""
                item.append(subject)
                try:
                    idcheck = itempt.index("ProQuest document ID ")
                    subject = itempt2[idcheck]
                except ValueError:
                    subject = ""
                item.append(subject)
                try:
                    idcheck = itempt.index("Source type ")
                    subject = itempt2[idcheck]
                except ValueError:
                    subject = ""
                item.append(subject)
        herf2 = source.find('a', {'id': 'addFlashPageParameterformat_fulltext'})
        if herf2 is not None:
            address = herf2['href']
            soup = BeautifulSoup(requests.get(address).text)
            text = soup.find('div', {'id': 'readableContent'})
            text= text.find_all('p')
            txt=""
            for i in text:
                txt = i.text.encode('utf-8')
                if txt is not None:
                    txt = txt.join(txt)
        else:
            txt =""
        item.append(txt)
        f = open('C:/Users/wang547/Downloads/test.txt', 'a')
        json.dump(item, f)
    time.sleep(10)
    next = browser.find_element_by_xpath('//*[@id="mainContentRight"]/nav/ul/li[13]/a')
    next.click()
    time.sleep(5)

