#python get index
from selenium import webdriver
from selenium.common.exceptions import TimeoutException
from selenium.common.exceptions import NoSuchElementException
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
import time
from bs4 import BeautifulSoup
import requests
import json
profile = webdriver.FirefoxProfile()
profile.set_preference("browser.download.folderList", 2)
profile.set_preference("browser.download.dir", "C:\Users\wang547\Downloads\Containdownload")
# profile.set_preference("browser.download.manager.showWhenStarting", False)
profile.set_preference("browser.helperApps.alwaysAsk.force", False)



browser = webdriver.Firefox(profile)

browser.get('https://search-proquest-com.libproxy2.usc.edu/?accountid=14749')
timeout = 5
try:

    search =browser.find_element_by_xpath('//*[@id="searchTerm"]')
    search.send_keys('FT(drone OR (Unmanned /N0 Aerial /N0 Vehicle) OR (Unmanned /N0 Air /N0 Vehicle) OR (Unmanned /N0 Aircraft /N0 Vehicle) OR (Unmanned /N0 Aerospace /N0 Vehicle) OR (Uninhabited /N0 Aircraft /N0 Vehicle)) AND PD(>2012-01-01 AND <2012-03-01)' )
    htmlElem = browser.find_element_by_xpath('//*[@id="expandedSearch"]')
    htmlElem.send_keys(Keys.ENTER)
    time.sleep(120)
except TimeoutException:
    print("Timed out waiting for page to load")

for i in range(0, 1000):
    print i
    soup = BeautifulSoup(browser.page_source)
    sources = soup.find_all('a', {'id': 'citationDocTitleLink'})
    for source in sources:
        item = []
        item.append(source['href'])
        f = open('C:/Users/wang547/Downloads/index1.txt', 'a')
        json.dump(item, f)
    next = browser.find_element_by_xpath('//*[@id="mainContentRight"]/nav/ul/li[13]/a')
    next.click()
    time.sleep(15)





#R code clearning
data<-readLines("C:\\Users\\wang547\\Downloads\\index1.txt")
output<-unlist(strsplit(data,"]"))
cleanid<-function(input){
add<-unlist(strsplit(input,"[[:punct:]]"))
id<-add[10]
class<-add[11]
account<-add[14]
output<-data.frame(id)
output<-cbind(output,class,account)
return(output)
}
library(plyr)
results<- lapply(output,cleanid)
results<-do.call(rbind,results)
write.table(results,file="index1.csv",row.names=FALSE,sep=",")

#Python code get index
from selenium import webdriver
from selenium.common.exceptions import TimeoutException
from selenium.common.exceptions import NoSuchElementException
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
import time
from bs4 import BeautifulSoup
import requests
import json
import csv
from xlrd import open_workbook

profile = webdriver.FirefoxProfile()
profile.set_preference("browser.download.folderList", 2)
profile.set_preference("browser.download.dir", "C:\Users\wang547\Downloads\Containdownload")
# profile.set_preference("browser.download.manager.showWhenStarting", False)
profile.set_preference("browser.helperApps.alwaysAsk.force", False)

book = open_workbook('C:\Users\wang547\Documents\index1.xlsx')

for i in range(1, 1000):
    print i
    id = int(book.sheet_by_index(0).cell(i, 0).value)
    print id
    address = book.sheet_by_index(0).cell(i, 1).value
    account = book.sheet_by_index(0).cell(i, 2).value
    url = 'https://search.proquest.com/docview/' + str(id) + '/' + 'abstract/' + str(address) + '/1?accountid=14749'
    print url
    soup = BeautifulSoup(requests.get(url).text)
    item = ["NA"]*6
    itemfile = soup.find_all('div', {'class': 'display_record_indexing_row'})
    itempt = []

    for itemf in itemfile:
        ifind = itemf.find('div', {'class': 'display_record_indexing_fieldname'})
        name = ifind.text.encode('utf-8')
        if name == "Subject ":
            subject = itemf.find('div', {'class': 'display_record_indexing_data'})
            subject = subject.text.encode('utf-8')
            item[0]=subject
        else:
            if name == "Publication date ":
                subject = itemf.find('div', {'class': 'display_record_indexing_data'})
                subject = subject.text.encode('utf-8')
                item[1] =subject
            else:

                if name == "Place of publication ":
                    subject = itemf.find('div', {'class': 'display_record_indexing_data'})
                    subject = subject.text.encode('utf-8')
                    item[2] =subject
                else:

                    if name == "ProQuest document ID ":
                        subject = itemf.find('div', {'class': 'display_record_indexing_data'})
                        subject = subject.text.encode('utf-8')
                        item[3] =subject
                    else:
                        if name == "Source type ":
                            subject = itemf.find('div', {'class': 'display_record_indexing_data'})
                            subject = subject.text.encode('utf-8')
                            item[4] =subject
    url2 = 'https://search.proquest.com/docview/' + str(id) + '/' + 'fulltext/' + str(address) + '/1?accountid=14749'
    soup = BeautifulSoup(requests.get(url2).text)
    text = soup.find('div', {'id': 'readableContent'})
    if text is not None:
        text = text.find_all('p')
        txt = ""
        for i in text:
            txt1 = i.text.encode('utf-8')
            if txt1 is not None:
                txt = txt + txt1
        item[5] =txt
    f = open('C:/Users/wang547/Downloads/test.txt', 'a')
    json.dump(item, f)
